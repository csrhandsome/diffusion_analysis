# view()需要连续的内存，并且不会复制数据 有点像reshape
import torch
x = torch.randn(4, 4)
y = x.view(16)  # 4x4 -> 16
y = x.view(-1)  # 自动计算维度
y = x.view(2, 8) # 4x4 -> 2x8
# 关键特性：
# 1. 要求张量是连续的(contiguous)
# 2. 共享原始数据的内存，修改会影响原数据
# 3. 如果张量不连续会报错
# 检查是否连续
print(x.is_contiguous())  # True or False
# 不连续的情况
x = torch.randn(4, 4)
y = x.transpose(0, 1)  # 转置后内存不连续
# y.view(16)  # 这里会报错



# flatten()更灵活，可以指定维度范围进行展平
x = torch.randn(4, 3, 2)
# 展平所有维度
y = x.flatten()  # 等同于x.flatten(0, -1)
# 只展平指定维度
y = x.flatten(1, 2)  # 只展平后两个维度：4 x 3 x 2 -> 4 x 6
# 关键特性：
# 1. 可以处理不连续的张量
# 2. 可以指定展平的维度范围
# 3. 会返回一个新的连续张量



# 1. 基本对比
def basic_comparison():
    # 创建一个示例张量
    x = torch.randn(2, 3, 4)  # [batch, channel, length]
    print("原始张量形状:", x.shape)  # torch.Size([2, 3, 4])

    # moveaxis: 移动维度位置
    x1 = torch.moveaxis(x, 1, -1)  
    print("moveaxis后:", x1.shape)  # torch.Size([2, 4, 3])

    # view: 重塑维度
    x2 = x.view(2, -1)  
    print("view后:", x2.shape)  # torch.Size([2, 12])

    # flatten: 展平指定维度
    x3 = x.flatten(1)  
    print("flatten后:", x3.shape)  # torch.Size([2, 12])


2024/10/31
在将任务从二维扩展到三维空间时，**action（动作）信息通常需要涵盖更多的维度和控制要素**。具体到你描述的使用夹爪抓取杯子的任务，action 信息可以包括以下几个方面：

1. **夹爪的运动控制**：
   - **位置变化**：在三维空间中，夹爪需要在三个轴（X, Y, Z）上移动，以定位到杯子的位置。
   - **姿态变化**：除了位置，夹爪的朝向（例如俯仰角、偏航角和滚转角）也是重要的，以确保能够正确夹持杯子。
   - **运动速度或加速度**（如果任务需要动态控制）：这可以帮助实现平滑的抓取动作，避免因过快移动而导致杯子滑落或摔落。

2. **夹爪的操作控制**：
   - **开合状态**：控制夹爪的开合程度，以适应不同大小的杯子。这不仅仅是简单的开/关，还可以包括开合的幅度，以实现精细化的抓取。
   - **抓取力度**（如果需要）：有些任务可能需要根据杯子的材质或重量调整抓取力度，以确保抓取稳固但不至于损坏物体。

3. **其他可能的动作参数**（视具体任务需求而定）：
   - **附加的控制参数**：例如，是否需要在抓取过程中施加额外的力或扭矩，以应对杯子的平衡或倾斜。
   - **时间参数**：某些动作可能需要在特定的时间窗口内完成，比如快速抓取或缓慢调整位置。

**综合起来，action 信息应包括**：
- **空间移动指令**：定位夹爪在三维空间中的目标位置和姿态。
- **夹爪操作指令**：控制夹爪的开合状态及其变化。
- **可能的力度或其他控制参数**：根据任务需求，调整抓取的力度或施加的力。

**举例**：
假设你使用的是笛卡尔空间控制，你的action 可以表示为一个六维向量，其中前三个分量表示夹爪在X, Y, Z轴上的位置变化，后三个分量表示夹爪的姿态变化（例如俯仰、偏航、滚转）。此外，可以加上一个额外的参数来表示夹爪的开合状态（如一个连续值从开到关）。

```plaintext
Action = [ΔX, ΔY, ΔZ, ΔRoll, ΔPitch, ΔYaw, Gripper_Closure]
```

这样的表示方式使得机器人能够同时控制夹爪的位置、姿态和抓取动作，从而更灵活地完成三维空间中的抓取任务。

---

**总结**：在三维空间中的夹取任务中，action 信息不仅包括夹爪的开合状态，还应涵盖夹爪在三维空间中的移动和姿态调整。因此，action 通常是位置和姿态指令与夹爪操作指令的组合。

**希望这些信息对你有所帮助！如果有更多具体问题，欢迎继续讨论。**

---

**参考资料**：
- *Robotics: Modelling, Planning and Control* by Bruno Siciliano et al.
- *Reinforcement Learning for Robotics* - various academic papers and tutorials.

---


# 2024.11.1
# 多模态融合？
# 视觉方案是不是要和文本方案分开？
# 加入多模态机制？
# 文本方案中的state和action如何定义？ -> state是iphone的位姿 action是夹爪的开合


# 2024.11.4(组会日)
# 真正的action应该包括了位姿数据加上夹爪的开合数据[posedata,angledata]  那么原有的state应该为?
# 是不是可以加上用扩散模型估计深度？


# 2024.11.5(英语pre日)
# 位姿在2D平面上由以下5个参数描述：位置 (2维)：x: 水平坐标 y: 垂直坐标 姿态 (3维)：θ(旋转角度) sx: x方向的缩放比例 sy: y方向的缩放比例
# however 位姿在3D空间中由以下6个参数描述：位置 (3维):x: x轴坐标 y: y轴坐标 z: z轴坐标 姿态 (3维) - 欧拉角表示 roll: 绕x轴旋转 pitch: 绕y轴旋转 yaw: 绕z轴旋转

# 2024.11.6()
action变成[posedata,angledata,force]
state变成[]
 
# 2024.11.7(万老师中期pre)
很酷的实现类的方法  model类（实现模型的init,forward）-->model policy(实现模型的 conditional_sample,predict_action,compute_loss) --> train_ _workspace(实现模型的train和eval过程)

# 2024.11.8(无事)
你的疑虑非常合理，理解输入和输出数据之间的关系对于设计有效的模型至关重要。让我们深入探讨一下在你的场景中，输入和输出包含重复（或类似）类型的数据是否合理，以及为什么这是一个常见且有效的做法。

1. 输入和输出的数据类型相同，但代表的含义不同

首先，虽然输入和输出可能包含相同类型的数据（如位姿、力、开合角度等），但它们通常代表不同的时间点或不同的角色。在你的案例中：

输入：当前时刻的机器人状态，包括当前的位姿、开合角度、力传感器数据，以及当前的环境感知（例如视频数据）。
输出：下一时刻机器人的目标状态，可能是要达到的位姿、预期的开合角度，以及可能的力反馈。
2. 时间序列预测中的常见做法

在时间序列预测或控制任务中，输入和输出包含相同类型的数据是非常常见的。例如，在机器人控制中，使用当前的状态信息来预测下一时刻的动作或状态：

当前状态（输入）：用于感知和理解当前的环境和自身状态。
未来状态（输出）：模型需要预测的下一步动作或状态，以实现目标任务。
这种设置下，虽然数据类型相同，但在时间维度上是不同的，输入和输出实际上并不重复。

3. 避免数据泄露（Data Leakage）

需要注意的是，模型设计时必须确保输入中不包含关于输出的未来信息，否则会导致数据泄露，模型可能在训练中“作弊”，无法正确地泛化到新数据。然而，在你的情况下，输入只包含当前时刻的信息，而输出是未来需要预测的状态，因而不存在数据泄露的问题。

4. 多模态数据融合的优势

将多种传感器数据（如视觉、位姿、力等）作为输入，可以让模型更全面地理解当前的状态和环境。这对于复杂的任务，如三维空间中的机器人操作，尤其重要。即使输入和输出包含相似的数据类型，模型仍需要学习从当前状态到未来目标状态的映射关系。

5. 具体到你的应用场景

在你的机器人项目中：

输入：
视频数据：提供环境的视觉信息，有助于理解周围环境和物体的位置、形状等信息。
当前位姿数据：机器人当前的空间位置和朝向。
机械手的开合角度：当前机械手的状态，是抓取操作的重要信息。
力传感器数据：当前承受的力，可以反映接触情况或环境阻力等。
输出：
下一步的运动位姿：机器人需要移动到的新位置和朝向。
下一个开合角度：机械手需要调整的开合程度，可能是为了抓取或释放物体。
预期的力反馈：根据任务需要，可能需要控制接触力，以防止损坏物体或保证稳定抓取。
在这个框架下，模型的任务是从当前的多模态感知输入，预测下一步需要执行的动作指令，以完成特定的任务。

6. 类似的实际应用案例

许多机器人学习和控制的研究都采用了类似的输入输出设计。例如：

模仿学习（Imitation Learning）：机器人学习从专家示范中模仿动作，输入是当前的感知和状态，输出是需要执行的动作。
强化学习（Reinforcement Learning）：机器人通过试错学习策略，输入是当前状态，输出是动作策略。
序列预测模型：在自然语言处理或时间序列预测中，模型会使用先前的词或数据点作为输入，预测下一个词或数据点。
7. 总结

重叠的数据类型并不代表重复的数据：关键是数据所处的时间点和所代表的意义不同。
模型需要学习的是从当前状态到未来状态的映射：即使输入和输出包含相同类型的数据，也是为了实现这一目的。
合理性：在遵循时间因果关系和避免数据泄露的前提下，这种设计是合理且有效的。
8. 建议

如果仍然担心数据的重复，可以考虑以下措施：

明确区分时间点：确保输入只包含当前时刻或过去的信息，输出是未来的预测。
输出动作指令而非直接的状态：例如，输出机器人的控制命令（如速度、加速度、关节角度的变化量等），而不是直接输出下一时刻的状态。
使用差分形式：模型输出预测的变化量，而非绝对值，这样可以强调模型预测的是从当前状态到下一状态的差异。



# 2024/11/21
多模态的数据对齐 因为频率的差距不大，因此准备强行对齐 simple_align 后续可能有更好的对齐方式


# 2024/11/22
原来的episodes_ends的数据的维度确实小很多  
类目！！！！ 终于把dataloader跑通了

# 2024/11/23
类目，train,test也跑通了
通过调整输入的dim，可以很好的跑通数据
现在目前来看结果好像不太理想，不知道是不是数据太少了，还有就是可能输入的数据还有很多没有用上。

# 2024/11/24 (分手了。。。。。。。。)
输入：当前时刻的机器人状态，包括当前的位姿、开合角度、力传感器数据，以及当前的环境感知（例如视频数据）。
输出：下一时刻机器人的目标状态，可能是要达到的位姿、预期的开合角度，以及可能的力反馈。

朝着上述目标努力，目前还需要
1. 根据输出的数据来跑通仿真（linux环境下）
2. 添加视频的数据（linux环境下）
3. 添加力的数据（要数据）
4. 添加声音的数据(数据处理)
5. 调整模型结构还有transformer（尝试多种模型）
6. 将RGB的视频数据和深度数据的对齐
# 2024/11/25
帮本科同学写了个读json的函数，可以直接读取json文件，返回字典
完成第4个工作(未完成)

# 2024/11/26
配置linux的环境 pytorch cuda mujoco 全部配置成功 牛逼！！！

# 2024/11/27 (究极休息日)
尝试第六个 
2可能可以了，但是Angle的数据师兄没有采上，所以数据还是对不齐
最终发现timestamp的对齐函数有问题,simple_align

# 2024/12/2
尝试利用开源的xml文件来进行模型的仿真
render（渲染）在程序中通常指的是将数据或场景转换为可视化图像的过程

# 2024/12/4(推胸50kg做组)
今天尝试简单的跑通仿真
在threedim_state_and_vision_test_model()函数中，需要一个评估的方法来暂停扩散的输出，这个方法暂时没写，先通过仿真来看

# 2024/12/6
尝试理解xml,正确的运用模型

# 2024/12/17
用双流网络统一rgb和深度数据
u-net是指
对称的编码器-解码器结构
逐步降采样和上采样
跳跃连接
特征维度先扩张后收缩

Sequential会自动按顺序执行,ModuleList需要手动执行


# 2024/12/21
Generalizable Humanoid Manipulation with  Improved 3D Diffusion Policies 说延伸预测的维度？

# 2024/12/24
可以建立一个dict，不带什么数据的版本对应什么维度对应什么训练，建立这样的关系，方便每次直接进行调用


“DiffusionPolicy在simulation和realworld中部署了相同的任务，证明了任务可以从虚拟迁移到现实。umi运用成熟的diffusionpolicy，通过简单的人类演示，成功的完成了很多现实当中的任务。3D-ViTac通过在机械夹具上面进行创新，在夹具上可视化了点云的数据，并且外置了LiDAR相机，生成了整体在运动中的点云数据。而在RDT-1B当中，首次引入了双臂机械臂和diffusion来协同在现实进行工作“
# 2024/12/27
假设在t=0时刻拍到一张图片
经过0.1秒的观测延迟
经过0.05秒的推理延迟
机器人需要0.15秒的执行延迟
那么实际上t=0时刻的图片对应的动作,最早要在t=0.3秒才能执行(大家都有延迟)
因此需要
预测未来序列：
模型不仅预测当前应该做什么
而是预测一个未来的动作序列
丢弃过时动作：
如果预测序列是 [a1, a2, a3, a4, a5]
而等到实际可以执行时，a1, a2 已经过时
那么直接从 a3 开始执行\

# 2024/12/30
要记得改成pybullet

visualencoder：
3D-vitac：
resnet 但权重改为r3m
diffusion policy & umi：
clip预训练的vit
resnet 但权重改为r3m
resnet
rdt:
clip,
siglip,
t5
# 2025/1/9
用动捕捕捉一组夹爪位姿的数据，拟合出一个函数曲线
Accelerate 自动管理设备（如 GPU、CPU、TPU），用户无需手动指定设备（如 .to(device)）自动处理数据并行（Data Parallelism）和模型并行（Model Parallelism）
自动处理数据并行（Data Parallelism）和模型并行（Model Parallelism）

每次训练或实验被视为一个 Run，Wandb 会记录该 Run 的所有相关信息（如超参数、指标、日志）提供直观的可视化界面

# 2025/1/10
hiddensize是指在模型当中的中间的size

# 2025/1/11
我悟了，为什么在模型内，数据的最后一个格式是统一的hiddensize或者outputsize了：
首先数据进入模型之前就要经过MLP的线性层进行维度的变化，变为统一的格式，在模型中更快的运算。
最后在输出的时候再次使用MLP进行decoder,又可以变回之前的格式。

# 2025/1/12（重要）
数据中的state和action不需要是重复相同的。
二维的推T中，state为通过仿真得出T形块在环境中的真实位置和方向，可以直接从物理引擎获取精确值。
在现实环境当中，需要通过动作捕捉装置来确定，所以这个维度需要根据动捕来待定。
在现实环境中，state还可以通过两个摄像机来确定。 
action可以是机械末端的操作指令，可以通过指令来完成操作。

非常怪异，action的维度和state的序列是一样的。
action这个动作序列可以直接就是位姿，知道了末端执行器的位姿就可以使用机械臂按照这个来了

# 2025/1/13(重要)
进行图像增强，包括颜色抖动和图像损坏，并在输入的本体感知中添加高斯噪声，信噪比（SNR）为40dB。我们还使用GPT-4-Turbo来增强和扩展语言指令

# 2025/1/14
在本文的机器人任务当中，在处理序列时：
q = 机器人需要操作的动作序列
k = ["给我拿一个红色小球", "帮我取一下蓝色方块", "拿住红色三角体"]
v = [红色小球视觉特征, 蓝色方块视觉特征,红色三角体视觉特征]

# 2025/1/17
CFG 通过引入一个“无条件”的生成路径（即不依赖条件的生成），并与条件生成路径结合，动态调整条件信号的影响强度。(但没用)
