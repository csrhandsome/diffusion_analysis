# config.yaml

# 模型配置
model:
  # Config for condition adpators
  lang_adaptor: mlp2x_gelu
  img_adaptor: mlp2x_gelu
  state_adaptor: mlp3x_gelu
  all_cond_adaptor: mlp4x_gelu
  lang_token_dim: 4096
  img_token_dim: 1152
  # Dim of action or proprioception vector
  # A `state` refers to an action or a proprioception vector
  # state的维度有待商榷，无法确定夹爪的state
  state_token_dim: 128
  # Config for RDT structure
  rdt:
    # 1B: num_head 32 hidden_size 2048
    hidden_size: 2048
    depth: 28
    num_heads: 32
    cond_pos_embed_type: multimodal 
  # For noise scheduler
  noise_scheduler:
    type: ddpm
    num_train_timesteps: 1000
    num_inference_timesteps: 5
    beta_schedule: squaredcos_cap_v2  # Critical choice
    prediction_type: sample
    clip_sample: False
  # For EMA (params averaging)
  # We do not use EMA currently
  ema:
    update_after_step: 0
    inv_gamma: 1.0
    power: 0.75
    min_value: 0.0
    max_value: 0.9999
  diffusion_policy:
    output_dim: 531  # 6+1+6+6+512
    input_dim: 7     # 6+1
    hidden_size: 1152
    depth: 28
    num_heads: 16
    horizon: 8
    max_lang_cond_len: 77
    img_cond_len: 512
    img_pos_embed_config:
    vision: 512

# 数据维度配置
data:
  vision_feature_dim: 512
  force_feature_dim: 1
  
  # 状态维度
  twodim_state_dim: 5
  threedim_state_dim: 520  # 6+1+1+512
  
  # 动作维度
  twodim_action_dim: 2
  threedim_action_dim: 8   # 6+1+1

# 时间步长配置
horizon:
  state_horizon: 2
  action_horizon: 8
  pred_horizon: 8

# 数据集配置
dataset:
  data_sum: 25650
  train_num: 100
  diffusion_num: 100
  lowdim_obs_dim: 2
  obs_dim: 514  # vision_feature_dim + lowdim_obs_dim
  zip_data_path: "data/pusht_cchi_v7_replay.zarr.zip"

# 训练配置
training:
  device: "cuda"
  dtype: "float32"
  batch_size: 32
  learning_rate: 1.0e-4
  weight_decay: 0.0
  warmup_steps: 1000
  total_steps: 100000
  
# 扩散模型配置
diffusion:
  num_diffusion_steps: 1000
  noise_schedule: "cosine"
  prediction_type: "epsilon"
  loss_type: "l2"

common:
  # The number of historical images
  img_history_size: 2
  # The number of future actions to predict
  action_chunk_size: 64
  # The number of cameras to be used in the model
  num_cameras: 3
  # Dimension for state/action, we use the same space for both state and action 同一个空间!!!!
  # This MUST be equal to configs/state_vec.py
  state_dim: 128


dataset:
  # We will extract the data from raw dataset
  # and store them in the disk buffer by producer
  # When training, we will read the data 
  # randomly from the buffer by consumer
  # The producer will replace the data which has been 
  # read by the consumer with new data

  # The path to the buffer (at least 400GB)
  buf_path: /path/to/buffer
  # The number of chunks in the buffer
  buf_num_chunks: 512
  # The number of samples (step rather than episode) in each chunk
  buf_chunk_size: 512

  # We will filter the episodes with length less than `epsd_len_thresh_low`
  epsd_len_thresh_low: 32
  # For those more than `epsd_len_thresh_high`,
  # we will randomly sample `epsd_len_thresh_high` steps each time we load the episode
  # to better balance the training datasets
  epsd_len_thresh_high: 2048
  # How to fit the image size
  image_aspect_ratio: pad
  # Maximum number of language tokens
  tokenizer_max_length: 1024